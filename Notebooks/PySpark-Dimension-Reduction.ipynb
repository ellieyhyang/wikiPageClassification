{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3acabdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from operator import add\n",
    "import psutil\n",
    "import time\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99a51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ec75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an array of size 20,000 with each position tells how many occurance of \n",
    "# the word in that position of the top-20-frequent-words dictionary\n",
    "def buildArray(listOfIndices):\n",
    "\n",
    "    returnVal = np.zeros(20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    \n",
    "    mysum = np.sum(returnVal)\n",
    "    \n",
    "    returnVal = np.divide(returnVal, mysum)\n",
    "    \n",
    "    return returnVal\n",
    "\n",
    "\n",
    "# # Check if the predicted label is a TP, FN, FP or TN\n",
    "# # Returns an array of len(4) containing binary values indicating the notation\n",
    "# # Ex: [0,1,0,0] means the prediction is a False Negative (FN)\n",
    "# def predictionNotation(true, pred):\n",
    "#     TP, FN, FP, TN = 0, 0, 0, 0\n",
    "    \n",
    "#     if true == 1:\n",
    "#         if pred == 1: \n",
    "#             TP = 1\n",
    "#         else:\n",
    "#             FN = 1\n",
    "#     elif true == 0:\n",
    "#         if pred == 1:\n",
    "#             FP = 1\n",
    "#         else:\n",
    "#             TN = 1\n",
    "    \n",
    "#     return np.array([TP, FN, FP, TN])\n",
    "\n",
    "# # calcuate the f1-score\n",
    "# def f_score(TP, FP, P):\n",
    "#     # f1 = NA if there is no predicted positive\n",
    "#     if TP == 0:\n",
    "#         f_score = 'NaN (zero TP)'\n",
    "#     else:\n",
    "#         recall = TP/P\n",
    "#         precision = TP/(TP+FP)\n",
    "#         f_score = 2*precision*recall / (precision+recall)\n",
    "    \n",
    "#     return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012d31a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 16:58:57 WARN Utils: Your hostname, Yihuis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.68.120 instead (on interface en0)\n",
      "21/12/22 16:58:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/12/22 16:58:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0cf6cb",
   "metadata": {},
   "source": [
    "# Data Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95d03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a regular expression here to check for non-letter characters\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# Load file into an RDD\n",
    "d_corpus = sc.textFile('SmallTrainingData.txt')\n",
    "\n",
    "# Transform both into a set of (docID, text) pairs\n",
    "d_keyAndText = d_corpus\\\n",
    "            .map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "\n",
    "# remove all non letter characters\n",
    "# Split the text in each (docID, text) pair into a list of words\n",
    "# Resulting RDD is a dataset with (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "d_keyAndListOfWords = d_keyAndText\\\n",
    "            .map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe0624",
   "metadata": {},
   "source": [
    "# Term Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fafad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# map (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = d_keyAndListOfWords\\\n",
    "            .map(lambda x: (x[1]))\\\n",
    "            .flatMap(lambda x: [w for w in x])\\\n",
    "            .map(lambda x: (x, 1))\n",
    "\n",
    "# count all of the words, giving --> (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "topWords = allCounts.top(20000, key=lambda x: x[1])\n",
    "topWords = np.array(topWords)\n",
    "\n",
    "# Create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 19999\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "topWordsK = sc.parallelize(range(20000))\n",
    "\n",
    "# Then, transform (0), (1), (2), ... to (\"MostCommonWord\", 1) (\"NextMostCommon\", 2), ...\n",
    "# the number will be the spot in the dictionary used to tell us where the word is located\n",
    "dictionary = topWordsK.map(lambda x: (topWords[x][0], x))\n",
    "\n",
    "# Will be using Map-Side Join Operation\n",
    "# Collect the small RDD as Map (a dict in python)\n",
    "dictionaryAsMap = dictionary.collectAsMap()\n",
    "\n",
    "# broad cast this to all worker nodes. \n",
    "sc.broadcast(dictionaryAsMap)\n",
    "\n",
    "# Create a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "allWordsWithDocID = d_keyAndListOfWords\\\n",
    "                    .flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "\n",
    "# Then do a simple map on it to get a set of (word, (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords = allWordsWithDocID\\\n",
    "                        .map(lambda x: (x[0], (x[1], dictionaryAsMap.get(x[0]))) \n",
    "                         if x[0] in dictionaryAsMap.keys() else None)\\\n",
    "                        .filter(lambda x: x!=None)\\\n",
    "                        .map(lambda x: (x[0], (x[1][1], x[1][0])))\n",
    "\n",
    "# Drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "# Create a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "# Converts the dictionary positions to a bag-of-words numpy array...\n",
    "# use the buildArray function to build the feature array\n",
    "# this gives a set of (docID, featureArray)\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc\\\n",
    "                        .map(lambda x: (x[0], buildArray(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3518d",
   "metadata": {},
   "source": [
    "# Reduce Feature Dimension Based on Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d06c824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store the number of docs in the training set\n",
    "numOfDocs = allDocsAsNumpyArrays.count() \n",
    "\n",
    "# calculate mean for each feature\n",
    "SumOfFeatures = allDocsAsNumpyArrays.map(lambda x: x[1]).reduce(lambda x, y: x+y)\n",
    "meanOfFeatures = SumOfFeatures / numOfDocs  # a vector of length 20k\n",
    "\n",
    "# calculate variance for each feature\n",
    "# Omit 'dividing by n' in calculation as it has no impact on the feature selection\n",
    "varianceOfFeatures = allDocsAsNumpyArrays\\\n",
    "                        .map(lambda x: x[1])\\\n",
    "                        .map(lambda x: (x - meanOfFeatures)**2)\\\n",
    "                        .reduce(lambda x, y: x+y)\n",
    "\n",
    "# Index of the top 10k features with highest variance\n",
    "top10kVarWordIdx = (-varianceOfFeatures).argsort()[:10000]\n",
    "\n",
    "\n",
    "# Make a subset of the selected features\n",
    "allDocsAsNumpyArrays_Reduced = allDocsAsNumpyArrays\\\n",
    "                                .map(lambda x: (x[0], x[1][top10kVarWordIdx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9763a8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AU35',\n",
       "  array([0.11425061, 0.06511057, 0.01781327, ..., 0.        , 0.        ,\n",
       "         0.        ]))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example reduced feature array for a document\n",
    "allDocsAsNumpyArrays_Reduced.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
